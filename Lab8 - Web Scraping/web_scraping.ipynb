{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cc32867-45cf-4916-ada7-53277b15c940",
   "metadata": {},
   "source": [
    "# Text Parsing & Web Scraping Using LangChain\n",
    "\n",
    "## Lab Description:\n",
    "\n",
    "In this lab, participants will learn how to perform web scraping using LangChain and process the extracted content with an LLM. We start by fetching text from a single webpage and generating a structured response. Then, we extend the process to iteratively scrape all links within a webpage. We then demonstrate how to extract and process text from a PDF embedded in a webpage. Finally, we explore the LangChain Wikipedia API wrapper to efficiently retrieve structured data from Wikipedia.\n",
    "\n",
    "## Lab Objectives:\n",
    "### After Completing the Lab, Participants will be able to:\n",
    "\n",
    "- Extract text from a single webpage and process it using an LLM to generate a structured and readable summary.\n",
    "- Iteratively scrape all links within a webpage and extract their content dynamically.\n",
    "- Scrape content from a PDF embedded in a webpage, demonstrating how to handle different document formats.\n",
    "- Use LangChain’s Wikipedia API wrapper to extract structured information from Wikipedia pages efficiently.\n",
    "\n",
    "## What is Web Scraping ?\n",
    "\n",
    "Web scraping is the automated process of extracting data from websites using scripts or bots. It typically involves fetching a webpage’s HTML and parsing specific content, such as text, images, or links, to gather desired information. Web scraping is an important process for collection of large text data for many applications like LLM Training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695bf08-34a9-4eb4-9b89-a13a25344df4",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b6edb12-2e41-4009-bd0d-33b0f8d0b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "import nest_asyncio\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d389543-5322-4a8f-8bc3-fef009c06ab2",
   "metadata": {},
   "source": [
    "## Loading the LLM\n",
    "\n",
    "We use **LLaMA 3.1:8b** model as the LLM for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d90689-3144-43cb-96ca-b01895c23f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_946/2214290135.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model='llama3.1:8b', base_url=\"http://10.79.253.114:11434\")\n"
     ]
    }
   ],
   "source": [
    "# Load the LLaMA 3.1 model with 8 billion parameters using the Ollama library\n",
    "llm = Ollama(model='llama3.1:8b', base_url=\"http://10.79.253.114:11434\")\n",
    "\n",
    "# Apply nest_asyncio to allow asynchronous tasks to run in the notebook environment\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904804a2-3293-4797-8a53-0f72309f3790",
   "metadata": {},
   "source": [
    "## Scraping from a single web page\n",
    "\n",
    "Scraping from a single webpage is a straightforward task. We simply provide the links that need to be scraped to LangChain's `AsyncChromiumLoader`.\n",
    "Chromium is one of the browsers supported by Playwright, a library used for browser automation. AsyncChromiumLoader uses an instance of Chromium in headless mode (which means the browser runs without displaying its graphical user interface). Essentially, it allows us to load webpages without needing to open a browser.\n",
    "\n",
    "BeautifulSoup is a library used to parse HTML content. It converts raw HTML into a structured tree, making navigation and manipulation easier.\n",
    "\n",
    "Once we obtain the HTML content of a webpage using `AsyncChromiumLoader`, we can pass it to the `BeautifulSoupTransformer`. Since BeautifulSoup simplifies working with HTML, it provides functionalities for extracting specific tags (such as `<span>`) from the document. We use the `tags_to_extract` parameter for this purpose.\n",
    "\n",
    "`docs_transformed` is a list of Document objects with associated metadata and page_content. Since we are primarily interested in the `page_content`, we can extract only that and store it in the document variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2751cab-d576-4bab-b765-336e30692822",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"flow.png\" alt=\"flow\" width=\"780\" height=\"620\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14590480-32c5-4fa1-8f89-4667784a3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load the CNN webpage in headless mode using AsyncChromiumLoader\n",
    "loader = AsyncChromiumLoader([\"https://edition.cnn.com/\"], headless=True)\n",
    "\n",
    "# Load the HTML content from the page\n",
    "html = loader.load()\n",
    "\n",
    "# Initialize BeautifulSoupTransformer for HTML processing\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "\n",
    "# Extract text from <span> tags in the HTML document\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])\n",
    "\n",
    "# Get the first 1000 characters of the extracted content\n",
    "document = docs_transformed[0].page_content[0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decb95f2-0864-4857-b79b-6ceb3299107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No   More Your CNN account Sign in to your CNN account Your CNN account Sign in to your CNN account  Follow CNN US-China trade talks Superbug threat Israeli airstrike on central Gaza school compound Trump-Carney Tesla sales plunge T. rex ancestors Frequent flyer millionaire  • Live Updates Live Updates Live Updates Pakistan says it downed 5 Indian fighter jets. India hasn’t confirmed losses, says it struck ‘terrorist infrastructure’ after Kashmir tourist massacre. Full report India launches strikes in Pakistan, Islamabad claims 5 Indian jets downed Military escalation between India and Pakistan spirals. Here’s what we know In pictures Scenes from Kashmir and Pakistan Indian PM Modi’s right-hand man ’proud of our armed forces‘ Video India conducts deepest strikes into Pakistan in more than 50 years 2:23 Trump’s team finally meeting with China. The future of the global economy is riding on its success India just agreed a massive trade deal – but it’s not with the US First boats carrying '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d1128-41cd-4d5f-b567-459f54df90b2",
   "metadata": {},
   "source": [
    "Now that we have the text content, its time to pass it to the LLM. The LLM can perform various tasks on the data. We can prompt the LLM to extract headlines from a news website and ask it to summarize it for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db96a582-8f82-4f98-9a20-aee8c98c57db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the results of extracting the headlines and summarizing the corresponding articles:\n",
       "\n",
       "**Article 1: US-China trade talks**\n",
       "\n",
       "* **Headline:** Trump’s team finally meeting with China. The future of the global economy is riding on its success\n",
       "* **Summary:** The US and China are holding crucial trade talks, which will determine the fate of the global economy. This development comes after India and Pakistan's military escalation.\n",
       "\n",
       "**Article 2: US-India-Pakistan Conflict**\n",
       "\n",
       "* **Headline:** Pakistan says it downed 5 Indian fighter jets. India hasn’t confirmed losses, says it struck ‘terrorist infrastructure’ after Kashmir tourist massacre.\n",
       "* **Summary:**\n",
       "\t+ Pakistan claims to have shot down 5 Indian fighter jets in a military escalation between the two nations.\n",
       "\t+ India has not confirmed any losses but claims to have targeted \"terrorist infrastructure\" following a recent attack on tourists in Kashmir.\n",
       "\t+ The conflict is ongoing, with both countries accusing each other of aggression.\n",
       "\n",
       "**Article 3: Tesla Sales**\n",
       "\n",
       "* **Headline:** Trump-Carney Tesla sales plunge\n",
       "* **Summary:** Due to the ongoing trade talks between the US and China, Tesla's sales have reportedly declined. However, it is unclear how this decline will affect the company in the long term.\n",
       "\n",
       "**Article 4: Superbug Threat**\n",
       "\n",
       "* **No corresponding headline found**\n",
       "\t+ (Assuming there was a mistake in parsing the HTML content or that the relevant article was not provided)\n",
       "\n",
       "**Article 5: Israeli Airstrike**\n",
       "\n",
       "* **Headline:** Israeli airstrike on central Gaza school compound\n",
       "* **Summary:** Israel carried out an airstrike on a school compound in central Gaza, sparking concerns over civilian casualties.\n",
       "\n",
       "**Article 6: India Launches Strikes**\n",
       "\n",
       "* **Headline:** India launches strikes in Pakistan, Islamabad claims 5 Indian jets downed\n",
       "* **Summary:**\n",
       "\t+ India has launched military strikes against targets in Pakistan.\n",
       "\t+ Pakistan claims to have shot down 5 Indian fighter jets during the operation.\n",
       "\t+ The conflict between the two nations continues to escalate.\n",
       "\n",
       "**Article 7: Frequent Flyer Millionaire**\n",
       "\n",
       "* **No corresponding headline found**\n",
       "\t+ (Assuming there was a mistake in parsing the HTML content or that the relevant article was not provided)\n",
       "\n",
       "Note: Articles 4 and 7 do not have corresponding headlines, which might indicate an error in parsing the HTML content."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Prompt / Instructions for the LLM\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are provided with the HTML content {content} of a news article webpage, which has been transformed using BeautifulSoup. The content includes multiple tech articles, each consisting of a headline and a corresponding body of text. Your task is to:\n",
    "\n",
    "        1. Identify the article headlines: Locate the headlines of the articles from the structured HTML.\n",
    "        2. Summarize the articles: After identifying a headline, find the corresponding body of the article that follows it, and summarize the key information from the article.\n",
    "        3. Respond in a structured format: Present the results in an organized and easy-to-read format with clear distinctions between each article.\n",
    "        \n",
    "        Output Requirements:\n",
    "\n",
    "        For each article, follow this structure:\n",
    "\n",
    "        Headline: Extract the headline of the article.\n",
    "        Summary: Summarize the corresponding article that follows the headline. Keep the summary concise, highlighting the most important points, including any key events, dates, or important figures mentioned.\n",
    "        \n",
    "        Ensure the response is easy to understand, using short paragraphs or bullet points when needed.\n",
    "        \n",
    "        Ignore any advertisements, sidebars, or unrelated content that may also be present on the page.\n",
    "\n",
    "        Ignore all the headers and other html elements, only focus on meaningful text. \n",
    "        \n",
    "        You should only extract content that is directly related to the news articles (headlines and the following article text).\n",
    "        \n",
    "        Keep your language simple and easy to follow to ensure clarity.\"\"\"\n",
    ")\n",
    "\n",
    "#Building the chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#Getting the response from the LLM\n",
    "response = chain.invoke({\"content\":document})\n",
    "\n",
    "#Displaying the response\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33509066-5fbc-4bb8-92a8-24ac83a7a829",
   "metadata": {},
   "source": [
    "The model fetches the important headlines, and summarizes it for us. All the data is scraped from the news website !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5ce8b-4fab-4e95-ab99-c64ba68100e2",
   "metadata": {},
   "source": [
    "## Scraping by iteratively fetching all the links in a given webpage\n",
    "\n",
    "So the expected question is, what if there are multiple links in a single webpage ? Apparently 90% of all websites today will have hyperlinks inside them. What if we want to fetch data from all these links ? or say, some of these links that are of interest to us ?. \n",
    "\n",
    "We can do this with BeautifulSoup and a little HTML knowledge. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d62e63-374b-4288-a1e3-10016a65f07b",
   "metadata": {},
   "source": [
    "## Fetching all the links within a website\n",
    "\n",
    "First we get the content of the website from which we need to fetch all the links. This can be done using python's built in requests library. Then we use BeautifulSoup to parse the HTML content.\n",
    "\n",
    "Once we have the parsed data, we find all the <a href> tags in the document. In HTML, The <a href> tag specifies the URL or location that the link points to. \n",
    "\n",
    "Now that we have all the <a href> tags, we can start to create URLS from them. \n",
    "\n",
    "If the text inside <a href> tag starts with \"https://\" we can understand that it is a completelty new URL and needs no further processing, we can add it to our url list. \n",
    "\n",
    "What if it starts with a \"/\" ? In this case we will have to append this text to our base url. \n",
    "\n",
    "Let us see an example:\n",
    "\n",
    "Suppose we have a dummy website. https://www.dummy.com\n",
    "\n",
    "It has an <a href> tag that looks like this: <a href = \"/dummy_example/example.pdf\">..</a>\n",
    "\n",
    "So we will have to add the \"/dummy_example/example.pdf\" to our base url, that is, \"https://www.dummy.com\", to get something like \"https://www.dummy.com/dummy_example/example.pdf\". \n",
    "\n",
    "It is then this url that we add to the url list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdac694-4ab9-4ea5-a2e2-e3ec0913fc79",
   "metadata": {},
   "source": [
    "1. **Imports and Initialization**:\n",
    "   - `requests` is used to handle HTTP requests, and `BeautifulSoup` from `bs4` is used for parsing HTML content.\n",
    "   - An empty list `url` is initialized to store the extracted URLs.\n",
    "\n",
    "2. **Fetching and Parsing HTML**:\n",
    "   - The base URL of the website is defined as `site`.\n",
    "   - An HTTP GET request fetches the HTML content, which is then parsed using `BeautifulSoup` with the `\"html.parser\"` option for efficient HTML parsing.\n",
    "\n",
    "3. **Extracting Links**:\n",
    "   - A loop iterates over each anchor tag (`<a>`) found on the page.\n",
    "   - For each anchor, the `href` attribute is checked to identify the URL format:\n",
    "     - If it starts with `\"https://\"`, it is considered an absolute URL and added directly to the `url` list.\n",
    "     - If it starts with `\"/\"`, it is treated as a relative URL and appended to the base site URL to form a complete link.\n",
    "     - Other formats are handled by appending them to the base URL with appropriate formatting.\n",
    "   \n",
    "This process collects all valid URLs, both absolute and relative, and stores them in the `url` list for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23176e0-4fd8-406c-9f4f-7bb887ca1107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://docs.ezmeral.hpe.com/unified-analytics/15/#content',\n",
       " 'https://docs.ezmeral.hpe.com/unified-analytics/15/index.html',\n",
       " 'https://docs.ezmeral.hpe.com/unified-analytics/15/GetStarted/get-started.html',\n",
       " 'https://docs.ezmeral.hpe.com/unified-analytics/15/ManageClusters/administration.html',\n",
       " 'https://docs.ezmeral.hpe.com/unified-analytics/15/Security/security.html',\n",
       " 'https://docs.ezmeral.hpe.com/unified-analytics/15/DataEngineering/data-engineering.html',\n",
       " 'https://docs.ezmeral.hpe.com/unified-analytics/15/DataAnalytics/data-analytics.html',\n",
       " 'https://docs.ezmeral.hpe.com/unified-analytics/15/DataScience/data-science.html',\n",
       " 'https://partner.hpe.com/',\n",
       " 'https://support.hpe.com/',\n",
       " 'https://developer.hpe.com/',\n",
       " 'https://community.hpe.com/t5/HPE-Ezmeral-Software-platform/bd-p/ezmeral-software-platform/',\n",
       " 'https://www.hpe.com/psnow/doc/a00109800enw',\n",
       " 'https://www.hpe.com/us/en/legal/privacy.html',\n",
       " 'https://docs.ezmeral.hpe.com/unified-analytics/15/./glossary/glossary.html']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#initialize the url list\n",
    "url = []   \n",
    "\n",
    "#the base url\n",
    "site = \"https://docs.ezmeral.hpe.com/unified-analytics/15/\"\n",
    "\n",
    "#get the content from the base url\n",
    "r = requests.get(site)\n",
    "#parsing using beautifulsoup\n",
    "s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "#iteratively fetch all the urls.\n",
    "for i in s.find_all(\"a\"):\n",
    "    href = i.attrs.get('href')\n",
    "\n",
    "    if href.startswith('https://'):\n",
    "        url.append(href)\n",
    "    elif href.startswith('/'):\n",
    "        if site.endswith('/'):\n",
    "            site = site[:-1]\n",
    "        url.append(site + href)\n",
    "        \n",
    "    else:\n",
    "        if site.endswith('/'):\n",
    "            url.append(site + href)\n",
    "        else:\n",
    "            url.append(site + '/' + href)    \n",
    "\n",
    "\n",
    "url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72747964-9258-45b6-9a1a-969e33f08a28",
   "metadata": {},
   "source": [
    "1. **Loading HTML Asynchronously**:\n",
    "   - `AsyncHtmlLoader` is used to load HTML content from the specified `url`.\n",
    "   - The `load()` method is called to retrieve the HTML content, storing it in the `html` variable.\n",
    "\n",
    "2. **Transforming HTML Content with BeautifulSoup**:\n",
    "   - A `BeautifulSoupTransformer` instance, `bs_transformer`, is created to handle HTML transformation.\n",
    "   - `transform_documents()` processes the `html` content, using BeautifulSoup to extract specified HTML tags.\n",
    "   - The `tags_to_extract` argument defines which HTML tags to extract—in this case, all `<div>` tags.\n",
    "\n",
    "This setup allows for asynchronous HTML loading and selective extraction of specified elements, making it efficient for targeted data retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6019cd39-3a26-4b7a-8a25-e6dc01fc8fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 8/8 [00:00<00:00, 18.61it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = AsyncHtmlLoader(url[0:8])\n",
    "html = loader.load()\n",
    "\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"div\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54911bb4-bfd4-4436-8dc4-471b5deac872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**HPE Ezmeral Unified Analytics Software Documentation**\n",
       "\n",
       "**Table of Contents**\n",
       "\n",
       "1. **Installation and Service Activation**\n",
       "\t* Upgrading HPE Ezmeral Unified Analytics Software\n",
       "2. **Cluster Management**\n",
       "\t* Expanding the Cluster\n",
       "\t* Importing Frameworks and Managing the Application Lifecycle\n",
       "3. **Connectivity and Storage**\n",
       "\t* Connecting to External S3 Object Stores\n",
       "\t* Connecting to HPE Ezmeral Data Fabric\n",
       "\t* Connecting to HPE GreenLake for File Storage\n",
       "4. **Configuration and Troubleshooting**\n",
       "\t* Configuring Endpoints\n",
       "\t* GPU Support\n",
       "\t* GPU Resource Management\n",
       "\t* Troubleshooting\n",
       "5. **Product Information**\n",
       "\t* Product Version and Lifecycle Support\n",
       "\t* Support Matrix\n",
       "6. **Release Notes**\n",
       "\t* Release Notes (1.5.0)\n",
       "\t* Release Notes (1.5.2)\n",
       "7. **Licensing and Support**\n",
       "\t* Term Licensing\n",
       "8. **Additional Resources**\n",
       "\t* Partners\n",
       "\t* Support\n",
       "\t* Dev-Hub\n",
       "\t* Community\n",
       "\t* Training\n",
       "\t* ALA\n",
       "\t* Privacy Policy\n",
       "\t* Glossary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful chat assistant. Given a chunk of text which is scraped text from an HTML document {doc} transformed using beautiful soup.\n",
    "       Identify headings of key topics from the text and summarize them in a structured format. Avoid any ads or other HTML content that might\n",
    "       be present. Only focus on relevant content that might be present.\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "doc = docs_transformed[1].page_content\n",
    "\n",
    "response = chain.invoke({\"doc\" : doc})\n",
    "\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4722ef5-5f12-46d5-848c-46fade085992",
   "metadata": {},
   "source": [
    "## Scraping From PDF document Inside a Website\n",
    "\n",
    "This code downloads a PDF document from a specified URL, extracts the text content from it, and then uses a language model to generate a summary. Here’s a step-by-step breakdown:\n",
    "\n",
    "1. **Import Libraries**: \n",
    "   - `requests` for handling the download of the PDF from the URL.\n",
    "   - `PyPDF2` for reading and extracting text from the PDF.\n",
    "\n",
    "2. **Download PDF**:\n",
    "   - Define the PDF URL (`site`), then use `requests.get` to download it.\n",
    "   - Save the content to a local file named \"doc.pdf\" in binary write mode (`wb`).\n",
    "\n",
    "3. **Extract Text from PDF**:\n",
    "   - Open \"doc.pdf\" in binary read mode (`rb`), create a `PdfReader` object to read the file, and initialize an empty string `text`.\n",
    "   - Loop through each page of the PDF, extract the text, and concatenate it to `text`.\n",
    "\n",
    "4. **Prompt Creation and Model Interaction**:\n",
    "   - Create a prompt template with `PromptTemplate.from_template` to generate a summarization request, inserting the extracted text (`text`) into the `{doc}` placeholder.\n",
    "   - Pass the prompt through a chain of processes, where `llm` represents the language model and `StrOutputParser` parses the model's output.\n",
    "   - The final output, `response`, is a summarized paragraph based on the content extracted from the PDF.\n",
    "\n",
    "5. **Display the Summary**:\n",
    "   - Use `display(Markdown(response))` to show the summary in a formatted Markdown output cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19dee8d-4834-4f19-82c5-d8394eb1511d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided PDF discusses various papers and research on neural machine translation systems, specifically focusing on Google's system and its ability to bridge the gap between human and machine translation. The document references several studies, including \"Deep recurrent models with fast-forward connections for neural machine translation\" by Jie Zhou et al. and \"Fast and accurate shift-reduce constituent parsing\" by Muhua Zhu et al. It also presents visualizations of attention mechanisms in a neural machine translation system, highlighting the ability of some attention heads to follow long-distance dependencies and perform anaphora resolution (i.e., resolving pronouns to their corresponding antecedents). The figures show the attentions for different words or groups of words, demonstrating how the model's attention mechanism can be used to understand complex sentence structures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "site = \"https://arxiv.org/pdf/1706.03762\"\n",
    "\n",
    "r = requests.get(site)\n",
    "\n",
    "with open(\"doc.pdf\", \"wb\") as file:\n",
    "    file.write(r.content)\n",
    "\n",
    "with open(\"doc.pdf\", \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given a text chunk extracted from a pdf document {doc}, summarize the content of the pdf into a single paragraph\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"doc\":text})\n",
    "\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d69a99-1eac-4966-b473-fbca37cc67d9",
   "metadata": {},
   "source": [
    "## Wikipedia API Wrapper\n",
    "\n",
    "To use this, ensure the `wikipedia` Python package is installed. This wrapper utilizes the Wikipedia API to perform searches and retrieve page summaries, typically returning summaries of the `top-k` results. It also restricts document content by setting a maximum character limit (`doc_content_chars_max`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bec136-c3dc-4695-8374-0d0f7c379c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: Hewlett Packard Enterprise\n",
      "Summary: The Hewlett Packard Enterprise Company (HPE) is an American multinational information technology company based in Spring, Texas.\n",
      "HPE was founded on November 1, 2015, in Palo Alto, California, as part of the splitting of the Hewlett-Packard company. It is a business-focused organization which works in servers, storage, networking, containerization software and consulting and support.\n",
      "The split was structured so that the former Hewlett-Packard Company would change its name to HP Inc. and spin off Hewlett Packard Enterprise as a newly created company. HP Inc. retained the old HP's personal computer and printing business, as well as its stock-price history and original NYSE ticker symbol for Hewlett-Packard; Enterprise trades under its own ticker symbol: HPE. At the time of the spin-off, HPE's revenue was slightly less than that of HP Inc.\n",
      "In 2017, HPE spun off its Enterprise Services business and merged it with Computer Sciences Corporation to become DXC Technology. Also in 2017, it spun off its software business segment and merged it with Micro Focus. Also in 2024, as part of the change in strategy, HPE's telecommunications business unit, the Communication Technology Group (CTG), was acquired by HCLTech for $225 million.\n",
      "HPE was ranked No. 107 in the 2018 Fortune 500 list of the largest United States corporations by total revenue.\n",
      "\n",
      "\n",
      "\n",
      "Page: Hewlett-Packard\n",
      "Summary: The Hewlett-Packard Company, commonly shortened to Hewlett-Packard ( HEW-lit PAK-ərd) or HP, was an American multinational information technology company headquartered in Palo Alto, California. HP developed and provided a wide variety of hardware components, as well as software and related services to consumers, small and medium-sized businesses (SMBs), and fairly large companies, including customers in government, health, and education sectors. The company was founded in a one-car garage in Palo Alto by Bill Hewlett and David Packard in 1939, and initially produced a line of electronic test and measurement equipment. The HP Garage at 367 Addison Avenue is now designated an official California Historical Landmark, and is marked with a plaque calling it the \"Birthplace of 'Silicon Valley'\".\n",
      "The company won its first big contract in 1938 to provide the HP 200B, a variation of its first product, the HP 200A low-distortion frequency oscillator for Walt Disney's production of the 1940 animated film Fantasia, which allowed Hewlett and Packard to formally establish the Hewlett-Packard Company on July 2, 1939. The company grew into a multinational corporation widely respected for its products. HP was the world's leading PC manufacturer from 2007 until the second quarter of 2013, when Lenovo moved ahead of HP. HP specialized in developing and manufacturing computing, data storage, and networking hardware; designing software; and delivering services. Major product lines included personal computing devices, enterprise and industry standard servers, related storage devices, networking products, software, and a range of printers and other imaging products. The company directly marketed its products to households; small- to medium-sized businesses and enterprises, as well as via online distribution; consumer-electronics and office-supply retailers; software partners; and major technology vendors. It also offered services and a consulting business for its products and partner products.\n",
      "In 1999, HP spun off its electronic and bio-analytical test and measurement instruments business into Agilent Technologies; HP retained focus on its later products, including computers and printers. It merged with Compaq in 2002, and acquired Electronic Data Systems in 2008, which led to combined revenues of $118.4 billion that year and a Fortune 500 ranking of 9 in 2009. In November 2009, HP announced its acquisition of 3Com, and closed the deal on April 12, 2010. On April 28, 2010, HP announced its buyout of Palm, Inc. for $1.2 billion. On September 2, 2010, \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wiki = WikipediaAPIWrapper(top_k_results=2, doc_content_char_max=500)\n",
    "\n",
    "print(wiki.run(query=\"hp enterprises\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff750ef-e246-449a-add6-4c9492d0679c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb5c63-4740-48e3-9e33-56cca590c591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
