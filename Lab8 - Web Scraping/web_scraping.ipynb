{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cc32867-45cf-4916-ada7-53277b15c940",
   "metadata": {},
   "source": [
    "# Text Parsing & Web Scraping Using LangChain\n",
    "\n",
    "## Lab Description:\n",
    "\n",
    "In this lab, participants will learn how to perform web scraping using LangChain and process the extracted content with an LLM. We start by fetching text from a single webpage and generating a structured response. Then, we extend the process to iteratively scrape all links within a webpage. We then demonstrate how to extract and process text from a PDF embedded in a webpage. Finally, we explore the LangChain Wikipedia API wrapper to efficiently retrieve structured data from Wikipedia.\n",
    "\n",
    "## Lab Objectives:\n",
    "### After Completing the Lab, Participants will be able to:\n",
    "\n",
    "- Extract text from a single webpage and process it using an LLM to generate a structured and readable summary.\n",
    "- Iteratively scrape all links within a webpage and extract their content dynamically.\n",
    "- Scrape content from a PDF embedded in a webpage, demonstrating how to handle different document formats.\n",
    "- Use LangChain’s Wikipedia API wrapper to extract structured information from Wikipedia pages efficiently.\n",
    "\n",
    "## What is Web Scraping ?\n",
    "\n",
    "Web scraping is the automated process of extracting data from websites using scripts or bots. It typically involves fetching a webpage’s HTML and parsing specific content, such as text, images, or links, to gather desired information. Web scraping is an important process for collection of large text data for many applications like LLM Training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695bf08-34a9-4eb4-9b89-a13a25344df4",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6edb12-2e41-4009-bd0d-33b0f8d0b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "import nest_asyncio\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d389543-5322-4a8f-8bc3-fef009c06ab2",
   "metadata": {},
   "source": [
    "## Loading the LLM\n",
    "\n",
    "We use **LLaMA 3.1:8b** model as the LLM for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31d90689-3144-43cb-96ca-b01895c23f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLaMA 3.1 model with 8 billion parameters using the Ollama library\n",
    "llm = Ollama(model='llama3.1:8b', base_url=\"http://10.79.253.112:11434\")\n",
    "\n",
    "# Apply nest_asyncio to allow asynchronous tasks to run in the notebook environment\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904804a2-3293-4797-8a53-0f72309f3790",
   "metadata": {},
   "source": [
    "## Scraping from a single web page\n",
    "\n",
    "Scraping from a single webpage is a straightforward task. We simply provide the links that need to be scraped to LangChain's `AsyncChromiumLoader`.\n",
    "Chromium is one of the browsers supported by Playwright, a library used for browser automation. AsyncChromiumLoader uses an instance of Chromium in headless mode (which means the browser runs without displaying its graphical user interface). Essentially, it allows us to load webpages without needing to open a browser.\n",
    "\n",
    "BeautifulSoup is a library used to parse HTML content. It converts raw HTML into a structured tree, making navigation and manipulation easier.\n",
    "\n",
    "Once we obtain the HTML content of a webpage using `AsyncChromiumLoader`, we can pass it to the `BeautifulSoupTransformer`. Since BeautifulSoup simplifies working with HTML, it provides functionalities for extracting specific tags (such as `<span>`) from the document. We use the `tags_to_extract` parameter for this purpose.\n",
    "\n",
    "`docs_transformed` is a list of Document objects with associated metadata and page_content. Since we are primarily interested in the `page_content`, we can extract only that and store it in the document variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2751cab-d576-4bab-b765-336e30692822",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"flow.png\" alt=\"flow\" width=\"780\" height=\"620\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14590480-32c5-4fa1-8f89-4667784a3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load the CNN webpage in headless mode using AsyncChromiumLoader\n",
    "loader = AsyncChromiumLoader([\"https://edition.cnn.com/\"], headless=True)\n",
    "\n",
    "# Load the HTML content from the page\n",
    "html = loader.load()\n",
    "\n",
    "# Initialize BeautifulSoupTransformer for HTML processing\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "\n",
    "# Extract text from <span> tags in the HTML document\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])\n",
    "\n",
    "# Get the first 1000 characters of the extracted content\n",
    "document = docs_transformed[0].page_content[0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decb95f2-0864-4857-b79b-6ceb3299107f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No   More Your CNN account Sign in to your CNN account Your CNN account Sign in to your CNN account  Follow CNN Jeff Bezos South Korean’s US adoption nightmare Elon Musk’s immigration path Timothée Chalamet lookalike contest Colin Farrell Ballon d’Or award winners Striking a balance between dire warnings and forward-looking optimism has become a defining challenge for Harris Live Updates Obama seizes on comedian’s Puerto Rico comments at Trump rally Fact Check Debunking 16 false claims Trump made at New York rally Video ‘That’s messed up’: Puerto Rican Trump voter reacts to controversial joke 2:59 Analysis Trump unveils the most extreme closing argument in modern presidential history Trump responds to comparisons of his rally to a 1939 pro-Nazi gathering at the same venue  • Video 1:08 Video Video shows ballots burning in Pacific Northwest states 1:08 Elon Musk details his own immigration path in response to criticism Jeff Bezos defends Washington Post non-endorsement after subscribers'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d1128-41cd-4d5f-b567-459f54df90b2",
   "metadata": {},
   "source": [
    "Now that we have the text content, its time to pass it to the LLM. The LLM can perform various tasks on the data. We can prompt the LLM to extract headlines from a news website and ask it to summarize it for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db96a582-8f82-4f98-9a20-aee8c98c57db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the results of extracting the headlines and summarizing the corresponding articles:\n",
       "\n",
       "**Article 1: Hurricane Milton**\n",
       "\n",
       "* **Headline:** Hurricane Milton unleashes its fury\n",
       "* **Summary:** A severe hurricane, named Milton, is causing significant damage along Florida's west coast. The storm has resulted in over 2 million people without power and has caused a portion of a stadium's roof to be ripped off in St. Petersburg. The article includes photos and videos showcasing the destruction.\n",
       "\n",
       "**Article 2: Hurricane Milton Updates**\n",
       "\n",
       "* **Headline:** See Milton’s projected path, rainfall and storm surge\n",
       "* **Summary:** This article provides an update on the current situation with Hurricane Milton. It includes a graphic showing the storm's projected path, rainfall totals, and storm surge predictions. The article also mentions that some residents are opting to stay in their homes despite the risks.\n",
       "\n",
       "**Article 3: Biden and Harris Address Hurricane Milton**\n",
       "\n",
       "* **Headline:** Biden and Harris step up public engagement about the dangers of Hurricane Milton\n",
       "* **Summary:** President Biden and Vice President Harris have spoken out about the dangers posed by Hurricane Milton. The article highlights their efforts to raise awareness about the storm's risks and encourages people to take necessary precautions.\n",
       "\n",
       "**Article 4: Video Reports**\n",
       "\n",
       "* **Headline:** CNN meteorologist shows where Milton is heading next\n",
       "* **Summary:** This article features a video report from a CNN meteorologist discussing the current trajectory of Hurricane Milton. The video provides an update on the storm's movement and includes footage of the damage caused by the hurricane.\n",
       "\n",
       "**Article 5: Video Reports**\n",
       "\n",
       "* **Headline:** Anderson Cooper shows what Milton’s storm surge looks like\n",
       "* **Summary:** This article features a video report from Anderson Cooper, where he describes what the storm surge caused by Hurricane Milton looks like. The video provides a visual representation of the destruction caused by the storm.\n",
       "\n",
       "**Article 6: Video Reports**\n",
       "\n",
       "* **Headline:** Tropicana Field roof blown off by Hurricane Milton’s powerful winds\n",
       "* **Summary:** This article features a short video showing the roof being blown off at Tropicana Field in St. Petersburg due to the strong winds from Hurricane Milton. The video is brief and provides a quick glimpse of the destruction.\n",
       "\n",
       "**Article 7: Nobel Prize**\n",
       "\n",
       "* **Headline:** Not available (there was no headline provided, but it's possible that this article was unrelated to the news section)\n",
       "* **Summary:** This article does not have a corresponding headline, so I will skip summarizing it.\n",
       "\n",
       "Note: Articles 8-10 are missing headlines and/or bodies of text."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Prompt / Instructions for the LLM\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are provided with the HTML content {content} of a news article webpage, which has been transformed using BeautifulSoup. The content includes multiple tech articles, each consisting of a headline and a corresponding body of text. Your task is to:\n",
    "\n",
    "        1. Identify the article headlines: Locate the headlines of the articles from the structured HTML.\n",
    "        2. Summarize the articles: After identifying a headline, find the corresponding body of the article that follows it, and summarize the key information from the article.\n",
    "        3. Respond in a structured format: Present the results in an organized and easy-to-read format with clear distinctions between each article.\n",
    "        \n",
    "        Output Requirements:\n",
    "\n",
    "        For each article, follow this structure:\n",
    "\n",
    "        Headline: Extract the headline of the article.\n",
    "        Summary: Summarize the corresponding article that follows the headline. Keep the summary concise, highlighting the most important points, including any key events, dates, or important figures mentioned.\n",
    "        \n",
    "        Ensure the response is easy to understand, using short paragraphs or bullet points when needed.\n",
    "        \n",
    "        Ignore any advertisements, sidebars, or unrelated content that may also be present on the page.\n",
    "\n",
    "        Ignore all the headers and other html elements, only focus on meaningful text. \n",
    "        \n",
    "        You should only extract content that is directly related to the news articles (headlines and the following article text).\n",
    "        \n",
    "        Keep your language simple and easy to follow to ensure clarity.\"\"\"\n",
    ")\n",
    "\n",
    "#Building the chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#Getting the response from the LLM\n",
    "response = chain.invoke({\"content\":document})\n",
    "\n",
    "#Displaying the response\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33509066-5fbc-4bb8-92a8-24ac83a7a829",
   "metadata": {},
   "source": [
    "The model fetches the important headlines, and summarizes it for us. All the data is scraped from the news website !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5ce8b-4fab-4e95-ab99-c64ba68100e2",
   "metadata": {},
   "source": [
    "## Scraping by iteratively fetching all the links in a given webpage\n",
    "\n",
    "So the expected question is, what if there are multiple links in a single webpage ? Apparently 90% of all websites today will have hyperlinks inside them. What if we want to fetch data from all these links ? or say, some of these links that are of interest to us ?. \n",
    "\n",
    "We can do this with BeautifulSoup and a little HTML knowledge. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d62e63-374b-4288-a1e3-10016a65f07b",
   "metadata": {},
   "source": [
    "## Fetching all the links within a website\n",
    "\n",
    "First we get the content of the website from which we need to fetch all the links. This can be done using python's built in requests library. Then we use BeautifulSoup to parse the HTML content.\n",
    "\n",
    "Once we have the parsed data, we find all the <a href> tags in the document. In HTML, The <a href> tag specifies the URL or location that the link points to. \n",
    "\n",
    "Now that we have all the <a href> tags, we can start to create URLS from them. \n",
    "\n",
    "If the text inside <a href> tag starts with \"https://\" we can understand that it is a completelty new URL and needs no further processing, we can add it to our url list. \n",
    "\n",
    "What if it starts with a \"/\" ? In this case we will have to append this text to our base url. \n",
    "\n",
    "Let us see an example:\n",
    "\n",
    "Suppose we have a dummy website. https://www.dummy.com\n",
    "\n",
    "It has an <a href> tag that looks like this: <a href = \"/dummy_example/example.pdf\">..</a>\n",
    "\n",
    "So we will have to add the \"/dummy_example/example.pdf\" to our base url, that is, \"https://www.dummy.com\", to get something like \"https://www.dummy.com/dummy_example/example.pdf\". \n",
    "\n",
    "It is then this url that we add to the url list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdac694-4ab9-4ea5-a2e2-e3ec0913fc79",
   "metadata": {},
   "source": [
    "1. **Imports and Initialization**:\n",
    "   - `requests` is used to handle HTTP requests, and `BeautifulSoup` from `bs4` is used for parsing HTML content.\n",
    "   - An empty list `url` is initialized to store the extracted URLs.\n",
    "\n",
    "2. **Fetching and Parsing HTML**:\n",
    "   - The base URL of the website is defined as `site`.\n",
    "   - An HTTP GET request fetches the HTML content, which is then parsed using `BeautifulSoup` with the `\"html.parser\"` option for efficient HTML parsing.\n",
    "\n",
    "3. **Extracting Links**:\n",
    "   - A loop iterates over each anchor tag (`<a>`) found on the page.\n",
    "   - For each anchor, the `href` attribute is checked to identify the URL format:\n",
    "     - If it starts with `\"https://\"`, it is considered an absolute URL and added directly to the `url` list.\n",
    "     - If it starts with `\"/\"`, it is treated as a relative URL and appended to the base site URL to form a complete link.\n",
    "     - Other formats are handled by appending them to the base URL with appropriate formatting.\n",
    "   \n",
    "This process collects all valid URLs, both absolute and relative, and stores them in the `url` list for further use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23176e0-4fd8-406c-9f4f-7bb887ca1107",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m     href \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     20\u001b[0m         url\u001b[38;5;241m.\u001b[39mappend(href)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#initialize the url list\n",
    "url = []   \n",
    "\n",
    "#the base url\n",
    "site = \"https://edition.cnn.com\"\n",
    "\n",
    "#get the content from the base url\n",
    "r = requests.get(site)\n",
    "#parsing using beautifulsoup\n",
    "s = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "#iteratively fetch all the urls.\n",
    "for i in s.find_all(\"a\"):\n",
    "    href = i.attrs.get('href')\n",
    "\n",
    "    if href.startswith('https://'):\n",
    "        url.append(href)\n",
    "    elif href.startswith('/'):\n",
    "        if site.endswith('/'):\n",
    "            site = site[:-1]\n",
    "        url.append(site + href)\n",
    "        \n",
    "    else:\n",
    "        if site.endswith('/'):\n",
    "            url.append(site + href)\n",
    "        else:\n",
    "            url.append(site + '/' + href)    \n",
    "\n",
    "\n",
    "url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72747964-9258-45b6-9a1a-969e33f08a28",
   "metadata": {},
   "source": [
    "1. **Loading HTML Asynchronously**:\n",
    "   - `AsyncHtmlLoader` is used to load HTML content from the specified `url`.\n",
    "   - The `load()` method is called to retrieve the HTML content, storing it in the `html` variable.\n",
    "\n",
    "2. **Transforming HTML Content with BeautifulSoup**:\n",
    "   - A `BeautifulSoupTransformer` instance, `bs_transformer`, is created to handle HTML transformation.\n",
    "   - `transform_documents()` processes the `html` content, using BeautifulSoup to extract specified HTML tags.\n",
    "   - The `tags_to_extract` argument defines which HTML tags to extract—in this case, all `<div>` tags.\n",
    "\n",
    "This setup allows for asynchronous HTML loading and selective extraction of specified elements, making it efficient for targeted data retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6019cd39-3a26-4b7a-8a25-e6dc01fc8fbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader \u001b[38;5;241m=\u001b[39m AsyncHtmlLoader(\u001b[43murl\u001b[49m[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m8\u001b[39m])\n\u001b[1;32m      2\u001b[0m html \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# html\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "loader = AsyncHtmlLoader(url)\n",
    "html = loader.load()\n",
    "\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"div\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54911bb4-bfd4-4436-8dc4-471b5deac872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**HPE Ezmeral Unified Analytics Software Documentation**\n",
       "\n",
       "**Table of Contents**\n",
       "\n",
       "1. **Installation and Service Activation**\n",
       "\t* Upgrading HPE Ezmeral Unified Analytics Software\n",
       "2. **Cluster Management**\n",
       "\t* Expanding the Cluster\n",
       "\t* Importing Frameworks and Managing the Application Lifecycle\n",
       "3. **Connectivity and Storage**\n",
       "\t* Connecting to External S3 Object Stores\n",
       "\t* Connecting to HPE Ezmeral Data Fabric\n",
       "\t* Connecting to HPE GreenLake for File Storage\n",
       "4. **Configuration and Troubleshooting**\n",
       "\t* Configuring Endpoints\n",
       "\t* GPU Support\n",
       "\t* GPU Resource Management\n",
       "\t* Troubleshooting\n",
       "5. **Product Information**\n",
       "\t* Product Version and Lifecycle Support\n",
       "\t* Support Matrix\n",
       "6. **Release Notes**\n",
       "\t* Release Notes (1.5.0)\n",
       "\t* Release Notes (1.5.2)\n",
       "7. **Licensing and Support**\n",
       "\t* Term Licensing\n",
       "8. **Additional Resources**\n",
       "\t* Partners\n",
       "\t* Support\n",
       "\t* Dev-Hub\n",
       "\t* Community\n",
       "\t* Training\n",
       "\t* ALA\n",
       "\t* Privacy Policy\n",
       "\t* Glossary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful chat assistant. Given a chunk of text which is scraped text from an HTML document {doc} transformed using beautiful soup.\n",
    "       Identify headings of key topics from the text and summarize them in a structured format. Avoid any ads or other HTML content that might\n",
    "       be present. Only focus on relevant content that might be present.\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "doc = docs_transformed[1].page_content\n",
    "\n",
    "response = chain.invoke({\"doc\" : doc})\n",
    "\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "556942b1-f773-4ee9-b1cd-22b7752c2968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|######################################################################################################| 8/8 [00:00<00:00, 43.04it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = AsyncHtmlLoader(url[0:8])\n",
    "html = loader.load()\n",
    "\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"div\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb660bbb-00a8-4165-b9a6-8bb52ec2ab25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a brief explanation of installing HPE Ezmeral Unified Analytics Software on user-provided hosts:\n",
       "\n",
       "To install HPE Ezmeral Unified Analytics Software on user-provided hosts, follow these steps:\n",
       "\n",
       "1. Run the installation script to access the installer web UI.\n",
       "2. Set up nodes, including selecting primary storage and setting up user authentication details.\n",
       "3. Configure tools & frameworks, such as choosing which ones to include or exclude.\n",
       "4. Specify storage details, including choosing a CA file for secure connections.\n",
       "5. Review your selections before proceeding with the installation.\n",
       "6. Submit the installation request to begin the installation process.\n",
       "\n",
       "After installation is complete, follow post-installation steps, which can be found in the provided documentation (pph-post-install.html)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful chat assistant. Given a chunk of text which is scraped text from an HTML document {doc} transformed using beautiful soup.\n",
    "       explain briefly about installation on user-provided hosts.\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "doc = docs_transformed[0].page_content\n",
    "\n",
    "response = chain.invoke({\"doc\" : doc})\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4722ef5-5f12-46d5-848c-46fade085992",
   "metadata": {},
   "source": [
    "## Scraping From PDF document Inside a Website\n",
    "\n",
    "This code downloads a PDF document from a specified URL, extracts the text content from it, and then uses a language model to generate a summary. Here’s a step-by-step breakdown:\n",
    "\n",
    "1. **Import Libraries**: \n",
    "   - `requests` for handling the download of the PDF from the URL.\n",
    "   - `PyPDF2` for reading and extracting text from the PDF.\n",
    "\n",
    "2. **Download PDF**:\n",
    "   - Define the PDF URL (`site`), then use `requests.get` to download it.\n",
    "   - Save the content to a local file named \"doc.pdf\" in binary write mode (`wb`).\n",
    "\n",
    "3. **Extract Text from PDF**:\n",
    "   - Open \"doc.pdf\" in binary read mode (`rb`), create a `PdfReader` object to read the file, and initialize an empty string `text`.\n",
    "   - Loop through each page of the PDF, extract the text, and concatenate it to `text`.\n",
    "\n",
    "4. **Prompt Creation and Model Interaction**:\n",
    "   - Create a prompt template with `PromptTemplate.from_template` to generate a summarization request, inserting the extracted text (`text`) into the `{doc}` placeholder.\n",
    "   - Pass the prompt through a chain of processes, where `llm` represents the language model and `StrOutputParser` parses the model's output.\n",
    "   - The final output, `response`, is a summarized paragraph based on the content extracted from the PDF.\n",
    "\n",
    "5. **Display the Summary**:\n",
    "   - Use `display(Markdown(response))` to show the summary in a formatted Markdown output cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19dee8d-4834-4f19-82c5-d8394eb1511d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided PDF discusses various papers and research on neural machine translation systems, specifically focusing on Google's system and its ability to bridge the gap between human and machine translation. The document references several studies, including \"Deep recurrent models with fast-forward connections for neural machine translation\" by Jie Zhou et al. and \"Fast and accurate shift-reduce constituent parsing\" by Muhua Zhu et al. It also presents visualizations of attention mechanisms in a neural machine translation system, highlighting the ability of some attention heads to follow long-distance dependencies and perform anaphora resolution (i.e., resolving pronouns to their corresponding antecedents). The figures show the attentions for different words or groups of words, demonstrating how the model's attention mechanism can be used to understand complex sentence structures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import PyPDF2\n",
    "\n",
    "site = \"https://arxiv.org/pdf/1706.03762\"\n",
    "\n",
    "r = requests.get(site)\n",
    "\n",
    "with open(\"doc.pdf\", \"wb\") as file:\n",
    "    file.write(r.content)\n",
    "\n",
    "with open(\"doc.pdf\", \"rb\") as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given a text chunk extracted from a pdf document {doc}, summarize the content of the pdf into a single paragraph\"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"doc\":text})\n",
    "\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d69a99-1eac-4966-b473-fbca37cc67d9",
   "metadata": {},
   "source": [
    "## Wikipedia API Wrapper\n",
    "\n",
    "To use this, ensure the `wikipedia` Python package is installed. This wrapper utilizes the Wikipedia API to perform searches and retrieve page summaries, typically returning summaries of the `top-k` results. It also restricts document content by setting a maximum character limit (`doc_content_chars_max`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bec136-c3dc-4695-8374-0d0f7c379c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: Hewlett Packard Enterprise\n",
      "Summary: The Hewlett Packard Enterprise Company (HPE) is an American multinational information technology company based in Spring, Texas.\n",
      "HPE was founded on November 1, 2015, in Palo Alto, California, as part of the splitting of the Hewlett-Packard company. It is a business-focused organization which works in servers, storage, networking, containerization software and consulting and support.\n",
      "The split was structured so that the former Hewlett-Packard Company would change its name to HP Inc. and spin off Hewlett Packard Enterprise as a newly created company. HP Inc. retained the old HP's personal computer and printing business, as well as its stock-price history and original NYSE ticker symbol for Hewlett-Packard; Enterprise trades under its own ticker symbol: HPE. At the time of the spin-off, HPE's revenue was slightly less than that of HP Inc.\n",
      "In 2017, HPE spun off its Enterprise Services business and merged it with Computer Sciences Corporation to become DXC Technology. Also in 2017, it spun off its software business segment and merged it with Micro Focus. Also in 2024, as part of the change in strategy, HPE's telecommunications business unit, the Communication Technology Group (CTG), was acquired by HCLTech for $225 million.\n",
      "HPE was ranked No. 107 in the 2018 Fortune 500 list of the largest United States corporations by total revenue.\n",
      "\n",
      "\n",
      "\n",
      "Page: Hewlett-Packard\n",
      "Summary: The Hewlett-Packard Company, commonly shortened to Hewlett-Packard ( HEW-lit PAK-ərd) or HP, was an American multinational information technology company headquartered in Palo Alto, California. HP developed and provided a wide variety of hardware components, as well as software and related services to consumers, small and medium-sized businesses (SMBs), and fairly large companies, including customers in government, health, and education sectors. The company was founded in a one-car garage in Palo Alto by Bill Hewlett and David Packard in 1939, and initially produced a line of electronic test and measurement equipment. The HP Garage at 367 Addison Avenue is now designated an official California Historical Landmark, and is marked with a plaque calling it the \"Birthplace of 'Silicon Valley'\".\n",
      "The company won its first big contract in 1938 to provide the HP 200B, a variation of its first product, the HP 200A low-distortion frequency oscillator for Walt Disney's production of the 1940 animated film Fantasia, which allowed Hewlett and Packard to formally establish the Hewlett-Packard Company on July 2, 1939. The company grew into a multinational corporation widely respected for its products. HP was the world's leading PC manufacturer from 2007 until the second quarter of 2013, when Lenovo moved ahead of HP. HP specialized in developing and manufacturing computing, data storage, and networking hardware; designing software; and delivering services. Major product lines included personal computing devices, enterprise and industry standard servers, related storage devices, networking products, software, and a range of printers and other imaging products. The company directly marketed its products to households; small- to medium-sized businesses and enterprises, as well as via online distribution; consumer-electronics and office-supply retailers; software partners; and major technology vendors. It also offered services and a consulting business for its products and partner products.\n",
      "In 1999, HP spun off its electronic and bio-analytical test and measurement instruments business into Agilent Technologies; HP retained focus on its later products, including computers and printers. It merged with Compaq in 2002, and acquired Electronic Data Systems in 2008, which led to combined revenues of $118.4 billion that year and a Fortune 500 ranking of 9 in 2009. In November 2009, HP announced its acquisition of 3Com, and closed the deal on April 12, 2010. On April 28, 2010, HP announced its buyout of Palm, Inc. for $1.2 billion. On September 2, 2010, \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wiki = WikipediaAPIWrapper(top_k_results=2, doc_content_char_max=500)\n",
    "\n",
    "print(wiki.run(query=\"hp enterprises\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff750ef-e246-449a-add6-4c9492d0679c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb5c63-4740-48e3-9e33-56cca590c591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
